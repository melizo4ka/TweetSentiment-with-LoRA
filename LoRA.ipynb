{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "354iyEmhWYRc",
    "tags": []
   },
   "source": [
    "# Transformer as a feature extractor and a classifier\n",
    "\n",
    "**TODO** In this project work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Using Transformer as Token Feature Extractor + External Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuH_jItJW0x3"
   },
   "source": [
    "- Choosing and preparing a dataset\n",
    "\n",
    "There are a lot of datasets that can be used as a base for this project, such as Sentiment140, TweetEval, etc. We will be using TweetEval since it is built specifically for evaluation of models on Twitter data. It contains around 58.000 tweets. \n",
    "\n",
    "- Preprocessing the text data\n",
    "\n",
    "Now we want to tokenize the input text using the tokenizer to convert it into input features.\n",
    "\n",
    "- Loading a Pre-trained Transformer (in our case DistilBERT)\n",
    "\n",
    "DistilBERT is a smaller, faster, and lighter version of BERT. It's key features are: reduced size, retained performance, transformer architecture.\n",
    "\n",
    "- Extracting features\n",
    "\n",
    "We pass the tokenized input through the transformer to get embeddings from the transformer.\n",
    "\n",
    "- Using the Embeddings with a Classifier and evaluating the baseline model\n",
    "\n",
    "Now we can use the embeddings as features for a traditional classifier, in this project we are using Logistic Regression. After training the classifier, we evaluate it on a test set using the usual metrics like accuracy, precision, recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 45615\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 12284\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# loading the TweetEval dataset\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")  \n",
    "\n",
    "# viewing the dataset structure\n",
    "print(dataset)\n",
    "\n",
    "n = 20000\n",
    "\n",
    "x_train = dataset['train']['text'][:n]\n",
    "x_test = dataset['test']['text'][:n]\n",
    "\n",
    "y_train = dataset['train']['label'][:n]\n",
    "y_test = dataset['test']['label'][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model_base = DistilBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(texts):\n",
    "    inputs = tokenizer_base(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_base(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # Use [CLS] token features\n",
    "\n",
    "train_features = extract_features(x_train)\n",
    "test_features = extract_features(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66      3972\n",
      "           1       0.66      0.67      0.67      5937\n",
      "           2       0.58      0.62      0.60      2375\n",
      "\n",
      "    accuracy                           0.65     12284\n",
      "   macro avg       0.64      0.64      0.64     12284\n",
      "weighted avg       0.65      0.65      0.65     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=100000)  # Increase max_iter if convergence issues arise\n",
    "clf.fit(train_features, y_train)\n",
    "\n",
    "y_pred = clf.predict(test_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check a couple of examples from the test set and see the results given by our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: @user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.\n",
      "True Label: 1, Predicted Label: 0\n",
      "--------------------------------------------------\n",
      "Text: OH: ‚ÄúI had a blue penis while I was this‚Äù [playing with Google Earth VR]\n",
      "True Label: 1, Predicted Label: 1\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def classify_text(text):\n",
    "    inputs = tokenizer_base([text], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        features = model_base(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    predicted_label = clf.predict(features)[0]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# to test we select a couple of samples from the test set\n",
    "num_samples = 2\n",
    "test_texts = dataset['test']['text'][:num_samples]\n",
    "test_labels = dataset['test']['label'][:num_samples]\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    predicted_label = classify_text(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Label: {test_labels[i]}, Predicted Label: {predicted_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is logical for the model to get the wrong label in about half of the cases since the average accuracy is 0.65. This accuracy can be improved by using the whole dataset or training for longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Fine-Tuning Transformer\n",
    "\n",
    "**TODO** Now, instead of using the transformer just as a feature extractor, we want to fine-tune it to handle both feature extraction and classification in an efficient manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# num_labels in our case is 3 since the possible sentiments are negative, neutral, positive\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,              # Rank for LoRA\n",
    "    lora_alpha=32,    # Scaling factor\n",
    "    lora_dropout=0.1, # Dropout for LoRA layers\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"]  # Modules to apply LoRA that correspond to \"query\", \"key\", \"value\" in DistilBERT\n",
    ")\n",
    "model_lora = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data01/pc24elimoc/miniconda3/envs/FML/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_353610/3441709534.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        torch.tensor(logits), torch.tensor(labels)\n",
    "    ).item()\n",
    "    return {\"eval_loss\": loss}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=300,\n",
    "    logging_steps=300,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=[\"wandb\"],\n",
    ")\n",
    "\n",
    "# initializing the trainer\n",
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melizaveta-mochalova\u001b[0m (\u001b[33melizaveta-mochalova-universita-firenze\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data01/pc24elimoc/Projects/PC/LBP/FML/wandb/run-20241106_110211-r52bt6du</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elizaveta-mochalova-universita-firenze/huggingface/runs/r52bt6du' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/elizaveta-mochalova-universita-firenze/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elizaveta-mochalova-universita-firenze/huggingface' target=\"_blank\">https://wandb.ai/elizaveta-mochalova-universita-firenze/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elizaveta-mochalova-universita-firenze/huggingface/runs/r52bt6du' target=\"_blank\">https://wandb.ai/elizaveta-mochalova-universita-firenze/huggingface/runs/r52bt6du</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data01/pc24elimoc/miniconda3/envs/FML/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1426' max='1426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1426/1426 06:25, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.048700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.932300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.889400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data01/pc24elimoc/miniconda3/envs/FML/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data01/pc24elimoc/miniconda3/envs/FML/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1426, training_loss=0.9502775130733367, metrics={'train_runtime': 388.6648, 'train_samples_per_second': 234.727, 'train_steps_per_second': 3.669, 'total_flos': 3036801251888640.0, 'train_loss': 0.9502775130733367, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I tollerate you!\n",
      "Predicted Label: 1\n"
     ]
    }
   ],
   "source": [
    "def classify_text(text):\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return predictions.item()\n",
    "\n",
    "\n",
    "# Test the Model with an Example\n",
    "example_text = \"I love this!\"\n",
    "predicted_label = classify_text(example_text)\n",
    "print(f\"Text: {example_text}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Numerical Programming and Reproducible Science.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
